{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD spark streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "postgres_pack = 'org.postgresql:postgresql:42.2.12'\n",
    "kafka_streams_pack = f\"org.apache.spark:spark-streaming-kafka-0-8_2.11:{os.environ['APACHE_SPARK_VERSION']}\"\n",
    "\n",
    "# Add extra packages\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f\"--packages {postgres_pack},{kafka_streams_pack} pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pyspark import SparkContext, Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load enviroments for connection to DB\n",
    "load_dotenv('../src/database.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_session(sparkConf):\n",
    "    if ('sparkSessionSingletonInstance' not in globals()):\n",
    "        globals()['sparkSessionSingletonInstance'] = SparkSession\\\n",
    "            .builder\\\n",
    "            .config(conf=sparkConf)\\\n",
    "            .getOrCreate()\n",
    "    return globals()['sparkSessionSingletonInstance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_df_to_postgres(spark_df):\n",
    "    \n",
    "    print(f\"=========-----> working with DB <-----=========\")\n",
    "    \n",
    "    try:\n",
    "        # From database.env.\n",
    "        db_host = os.environ['POSTGRES_HOST']\n",
    "        db_name = os.environ['POSTGRES_DB']\n",
    "        db_user = os.environ['POSTGRES_USER']\n",
    "        db_pass = os.environ['POSTGRES_PASSWORD']\n",
    "        db_table = 'clickstream_filtered'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"--> It seems Database not initialized from .env\", e)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # '?stringtype=unspecified' For personalized enum_type in postgres\n",
    "        spark_df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"driver\", 'org.postgresql.Driver') \\\n",
    "            .option(\"url\", f\"jdbc:postgresql://{db_host}:5432/{db_name}?stringtype=unspecified\") \\\n",
    "            .option(\"dbtable\", db_table) \\\n",
    "            .option(\"user\", db_user) \\\n",
    "            .option(\"password\", db_pass) \\\n",
    "            .save()\n",
    "        print(f\"=========-----> Insert complete <-----=========\")\n",
    "    except Exception as e:\n",
    "        print(\"--> It seems an Error with connection to DB\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(time, rdd):\n",
    "    \n",
    "    print(f\"===========-----> {str(time)} <-----===========\")\n",
    "\n",
    "    result_df = None\n",
    "    filtered_list = ['Click', 'Like', 'Complete']\n",
    "    \n",
    "    try:\n",
    "        spark = get_spark_session(rdd.context.getConf())\n",
    "        \n",
    "        row_rdd = rdd \\\n",
    "                .map(lambda r_json: Row(epk_id=r_json['epk_id'],\n",
    "                                        content_id=r_json['content_id'],\n",
    "                                        event_type=r_json['event_type'],\n",
    "                                        event_ts=r_json['event_ts'],\n",
    "                                        insert_ts=r_json['insert_ts'])) \\\n",
    "                .filter(lambda row: row['event_type'] in filtered_list)\n",
    "                                       \n",
    "        result_df = spark.createDataFrame(row_rdd)\n",
    "        result_df.createOrReplaceTempView(\"treasury_stream\")\n",
    "        result_df.show(n=3)\n",
    "        \n",
    "        spark_df_to_postgres(result_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"--> Opps! Is seems an Error!!!\", e)\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(kafka_server, kafka_topic):\n",
    "\n",
    "    sc = SparkContext(appName=\"PythonStreamingKafka\")\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    n_seconds = 5\n",
    "    ssc = StreamingContext(sc, n_seconds) #job every n_seconds\n",
    "\n",
    "    try:\n",
    "        directKafkaStream = KafkaUtils.createDirectStream(ssc,\n",
    "                                        [kafka_topic],\n",
    "                                        {\"metadata.broker.list\": kafka_server})\n",
    "    except:\n",
    "        raise ConnectionError(f\"Kafka error: Connection refused: \\\n",
    "                            broker_list={kafka_server} topic={kafka_topic}\")\n",
    "        \n",
    "    parsed_lines = directKafkaStream.map(lambda v: json.loads(v[1]))\n",
    "\n",
    "    # RDD handling\n",
    "    filtered_df = parsed_lines.foreachRDD(data_processing)\n",
    "\n",
    "    spark_df_to_postgres(filtered_df)\n",
    "\n",
    "    return ssc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========-----> working with DB <-----=========\n",
      "--> It seems an Error with connection to DB 'NoneType' object has no attribute 'write'\n",
      "===========-----> 2020-04-08 09:05:50 <-----===========\n",
      "+----------+------+----------+----------+-------------------+\n",
      "|content_id|epk_id|  event_ts|event_type|          insert_ts|\n",
      "+----------+------+----------+----------+-------------------+\n",
      "|      2242|  5578|1586254465|     Click|1.586336746999377E9|\n",
      "|       465|  5748|1586007754|  Complete|1.586336747020188E9|\n",
      "|      2119|  8943|1585388642|     Click| 1.58633674700369E9|\n",
      "+----------+------+----------+----------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "=========-----> working with DB <-----=========\n",
      "=========-----> Insert complete <-----=========\n",
      "===========-----> 2020-04-08 09:05:55 <-----===========\n",
      "+----------+------+----------+----------+-------------------+\n",
      "|content_id|epk_id|  event_ts|event_type|          insert_ts|\n",
      "+----------+------+----------+----------+-------------------+\n",
      "|      2809|  8448|1585883364|  Complete|1.586336750134334E9|\n",
      "|      3999|  1579|1585308182|  Complete|1.586336753231109E9|\n",
      "|      3184|  2402|1585522778|  Complete|1.586336753170254E9|\n",
      "+----------+------+----------+----------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "=========-----> working with DB <-----=========\n",
      "=========-----> Insert complete <-----=========\n",
      "===========-----> 2020-04-08 09:06:00 <-----===========\n",
      "+----------+------+----------+----------+-------------------+\n",
      "|content_id|epk_id|  event_ts|event_type|          insert_ts|\n",
      "+----------+------+----------+----------+-------------------+\n",
      "|      2939|  7476|1586306342|  Complete|1.586336756282743E9|\n",
      "|      1424|  1445|1585221145|     Click|1.586336759351786E9|\n",
      "|      3096|  2418|1586290887|     Click|1.586336759370149E9|\n",
      "+----------+------+----------+----------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "=========-----> working with DB <-----=========\n",
      "=========-----> Insert complete <-----=========\n"
     ]
    }
   ],
   "source": [
    "server = 'kafka:9093'\n",
    "topic = 'clickstream'\n",
    "output_path = '/tmp/spark/checkpoint_01'\n",
    "\n",
    "ssc = StreamingContext.getOrCreate(output_path, lambda: create_context(server, topic))\n",
    "ssc.start()\n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rdf /tmp/spark/checkpoint_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['sparkSessionSingletonInstance'].stop()\n",
    "del(globals()['sparkSessionSingletonInstance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName('test')\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([1, 2, 3], \"int\").toDF(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
