{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD spark streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \\\n",
    "        org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.5 \\\n",
    "        pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark import SparkContext, Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_session(sparkConf):\n",
    "    if ('sparkSessionSingletonInstance' not in globals()):\n",
    "        globals()['sparkSessionSingletonInstance'] = SparkSession\\\n",
    "            .builder\\\n",
    "            .config(conf=sparkConf)\\\n",
    "            .getOrCreate()\n",
    "    return globals()['sparkSessionSingletonInstance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(time, rdd):\n",
    "    \n",
    "    print(f\"===========-----> {str(time)} <-----===========\")\n",
    "\n",
    "    try:\n",
    "        spark = get_spark_session(rdd.context.getConf())\n",
    "        \n",
    "        filtered_list = ['Click', 'Like', 'Complete']\n",
    "\n",
    "        row_rdd = rdd \\\n",
    "                .map(lambda r_json: Row(epk_id=r_json['epk_id'],\n",
    "                                        content_id=r_json['content_id'],\n",
    "                                        event_type=r_json['event_type'],\n",
    "                                        event_ts=r_json['event_ts'],\n",
    "                                        insert_ts=r_json['insert_ts'])) \\\n",
    "                .filter(lambda row: row['event_type'] in filtered_list)\n",
    "                                       \n",
    "        result_df = spark.createDataFrame(row_rdd)\n",
    "        result_df.createOrReplaceTempView(\"treasury_stream\")\n",
    "\n",
    "        result_df.show(n=3)\n",
    "\n",
    "        # Insert into DB\n",
    "        try:\n",
    "            # From database.env\n",
    "            db_host = os.environ['POSTGRES_HOST']\n",
    "            db_name = os.environ['POSTGRES_DB']\n",
    "            db_user = os.environ['POSTGRES_USER']\n",
    "            db_pass = os.environ['POSTGRES_PASSWORD']\n",
    "            db_table = 'clickstream_filtered'\n",
    "            \n",
    "            testResultDataFrame.write \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"driver\", 'org.postgresql.Driver') \\\n",
    "                .option(\"url\", f\"jdbc:postgresql://{db_host}:5432/{db_name}\") \\\n",
    "                .option(\"dbtable\", db_table) \\\n",
    "                .option(\"user\", db_user) \\\n",
    "                .option(\"password\", db_pass) \\\n",
    "                .save()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"--> Opps! It seems an Errrorrr with DB working!\", e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"--> Opps! Is seems an Error!!!\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(kafka_server, kafka_topic):\n",
    "\n",
    "    sc = SparkContext(appName=\"PythonStreamingKafka\")\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    ssc = StreamingContext(sc, 5)\n",
    "\n",
    "    try:\n",
    "        directKafkaStream = KafkaUtils.createDirectStream(ssc,\n",
    "                                        [kafka_topic],\n",
    "                                        {\"metadata.broker.list\": kafka_server})\n",
    "    except:\n",
    "        raise ConnectionError(f\"Kafka error: Connection refused: \\\n",
    "                            broker_list={kafka_server} topic={kafka_topic}\")\n",
    "        \n",
    "    parsed_lines = directKafkaStream.map(lambda v: json.loads(v[1]))\n",
    "\n",
    "    # RDD handling\n",
    "    parsed_lines.foreachRDD(data_processing)\n",
    "\n",
    "    return ssc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========-----> 2020-04-07 15:19:25 <-----===========\n",
      "--> Opps! Is seems an Error!!! RDD is empty\n",
      "===========-----> 2020-04-07 15:19:30 <-----===========\n",
      "--> Opps! Is seems an Error!!! RDD is empty\n",
      "===========-----> 2020-04-07 15:19:35 <-----===========\n",
      "+----------+------+----------+----------+-------------------+\n",
      "|content_id|epk_id|  event_ts|event_type|          insert_ts|\n",
      "+----------+------+----------+----------+-------------------+\n",
      "|      3433|   411|1586097750|     Click|1.586272772301876E9|\n",
      "|      3425|  3461|1585097759|     Click|1.586272772313592E9|\n",
      "|       811|  7749|1585110764|     Click|1.586272772315791E9|\n",
      "+----------+------+----------+----------+-------------------+\n",
      "\n",
      "--> Opps! It seems an Errrorrr with DB working! 'POSTGRES_HOST'\n",
      "===========-----> 2020-04-07 15:19:40 <-----===========\n",
      "+----------+------+----------+----------+-------------------+\n",
      "|content_id|epk_id|  event_ts|event_type|          insert_ts|\n",
      "+----------+------+----------+----------+-------------------+\n",
      "|      1872|  3955|1586269349|  Complete|1.586272775338701E9|\n",
      "|       285|  1608|1586242337|     Click|1.586272775356217E9|\n",
      "|        82|  3237|1585926578|     Click|1.586272775370792E9|\n",
      "+----------+------+----------+----------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "--> Opps! It seems an Errrorrr with DB working! 'POSTGRES_HOST'\n",
      "===========-----> 2020-04-07 15:19:45 <-----===========\n",
      "+----------+------+----------+----------+-------------------+\n",
      "|content_id|epk_id|  event_ts|event_type|          insert_ts|\n",
      "+----------+------+----------+----------+-------------------+\n",
      "|      3319|  9035|1585478780|     Click|1.586272781515572E9|\n",
      "|      2297|  6354|1585385849|     Click|1.586272784582411E9|\n",
      "|      2748|  2477|1585586613|     Click|1.586272784585231E9|\n",
      "+----------+------+----------+----------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "--> Opps! It seems an Errrorrr with DB working! 'POSTGRES_HOST'\n",
      "===========-----> 2020-04-07 15:19:50 <-----===========\n",
      "+----------+------+----------+----------+-------------------+\n",
      "|content_id|epk_id|  event_ts|event_type|          insert_ts|\n",
      "+----------+------+----------+----------+-------------------+\n",
      "|      1027|  2147|1586116095|     Click|1.586272787623947E9|\n",
      "|      1571|  4598|1585877656|  Complete|1.586272787645925E9|\n",
      "|      2285|  2737|1585450289|     Click| 1.58627278760185E9|\n",
      "+----------+------+----------+----------+-------------------+\n",
      "\n",
      "--> Opps! It seems an Errrorrr with DB working! 'POSTGRES_HOST'\n"
     ]
    }
   ],
   "source": [
    "server = 'kafka:9093'\n",
    "topic = 'clickstream'\n",
    "output_path = '/tmp/spark/checkpoint_01'\n",
    "\n",
    "ssc = StreamingContext.getOrCreate(output_path, lambda: create_context(server, topic))\n",
    "ssc.start()\n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rdf /tmp/spark/checkpoint_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['sparkSessionSingletonInstance'].stop()\n",
    "del(globals()['sparkSessionSingletonInstance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
